<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: functional-testing | Jake Trent]]></title>
  <link href="http://jaketrent.com/blog/categories/functional-testing/atom.xml" rel="self"/>
  <link href="http://jaketrent.com/"/>
  <updated>2015-05-20T09:33:33-06:00</updated>
  <id>http://jaketrent.com/</id>
  <author>
    <name><![CDATA[Jake Trent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Destroy Duplicate Tests]]></title>
    <link href="http://jaketrent.com/post/destroy-duplicate-tests/"/>
    <updated>2014-11-05T16:59:00-07:00</updated>
    <id>http://jaketrent.com/post/destroy-duplicate-tests</id>
    <content type="html"><![CDATA[<p>As soon as we begin to write a test for our code, it is natural for us to think that we are doing a good thing, and often, we are.  Yet, I believe there are times that we’re writing tests when we’re hurting more than helping — and, of course, this is not on purpose.  To clarify, I’m an advocate for testing in general, and this is a short thought on how to make it better.</p>

<p><img src="http://i.imgur.com/ozzuTNQ.png" alt="Double tests are not fine" /></p>

<!--more-->


<p>As soon as we begin to write a test for our code, it is natural for us to think that we are doing a good thing, and often, we are.  Yet, I believe there are times that we’re writing tests when we’re hurting more than helping — and, of course, this is not on purpose.  To clarify, I’m an advocate for testing in general, and this is a short thought on how to make it better.</p>

<h2>Verify It, and Be Done</h2>

<p>One of the main goals of testing is to verify that what you have written is correct.  So, if we’ve met that goal, there’s no need to go around the track one more time and see the checkered flag again.  The second time around produces no extra glory and no extra benefit.</p>

<p>If we cover a section of code many times, it isn’t more helpful than the first time we covered it.  To verify twice isn’t to verify any better.  If the second attempt does happen to verify the same thing in an obviously better way, remove the first attempt and keep the second.</p>

<p>If it’s a variation of a certain case that you’re verifying, that’s different.  Adding new cases based on slight permutations of previous cases can be a good thing.  But covering the exact same thing provides no value.  In fact, multiple verification of code is just a type of debt.  It should be a smell in your test code that alerts you to clean things up.</p>

<h2>The Debt of Duplicate Tests</h2>

<p>If you have multiples of something, it just increases the maintenance over time.  Why would you want to update two tests instead of one?  Now that you have duplicate tests, you also have to keep them in sync.  Of course, if they cover the exact same case, if you change source code to fix the one test, the other will still be broken and be apparent and easy to fix.</p>

<p>The more tests you have, the longer your feedback loop in development or in a continuous build environment will be.  Multiply that extra wait time across your life on the project, and it has the possibility of being a non-trivial product.  Of course we need to wait for the tests that are needful and provide added value, but we shouldn’t wait needlessly.</p>

<p>Sometimes you do see duplicate tests within the same file — for instance, within the same unit.  This might happen when different developers approach the unit at different times to add tests.</p>

<p>I think it’s probably more often the case that duplicate tests are found across test classes -- meaning across the different types of tests.  For instance, a developer might write a unit test that covers a case.  Later, someone else might add an integration test that adds the same case.  Later still, someone else might add a functional test that adds the same case yet again.  All these developers are well-intentioned in adding tests.  They all need to think, communicate, investigate, and coordinate a little more to avoid the duplicate test problem.</p>

<h2>Deleting Duplicate Tests</h2>

<p>When duplicate tests are found, we should delete them.  Again, this might require some thinking.  We might want to consider which of the duplicate cases is the best test and therefore the one to keep.  This consideration might include which test is most stable, runs the fastest, is most readable, best designed, latest, earliest, etc.</p>

<h2>Avoiding Duplicate Tests</h2>

<p>The best scenario would be the one where we avoid duplicate tests.  Teams with clear guidelines will be able to coordinate better.  Useful information might include which classes of tests exist in the project and what each is intended for.  We might describe which kinds of tests we prefer, in which order, for certain kinds of verifications.  Having clean, well-organized tests will also encourage the team to read each others’ tests and familiarize themselves with what’s already written and know where to find existing cases and where to properly categorize new cases.</p>

<p>So have fun testing, and destroy the duplicate tests!  Yay for test doubles, but boo for double tests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Release It! Stability Review]]></title>
    <link href="http://jaketrent.com/post/release-it-stability-review/"/>
    <updated>2009-08-18T07:37:00-06:00</updated>
    <id>http://jaketrent.com/post/release-it-stability-review</id>
    <content type="html"><![CDATA[<p>I recently read Release It! by Michael Nygard.  I became interested in this book when the NFJS 2008 panel cited it as their #1 suggested read.  In my view, the essence of Nygard's expose boils down to two points:</p>

<ol>
<li>One can build software that passes QA with flying colors and still fails miserably in real environments.</li>
<li>Problem in production are unavoidable. Good software will be able to navigate them as gracefully as possible.</li>
</ol>


<p>That said, the book was mostly conceptual, offering real-world examples of how antipatterns in software development made problems that were encountered even worse.  Then, he counters with patterns in stability, capacity, general design, and operations.  The principles discussed are those that most developers have at least some exposure to, but those that we don't necessarily consider every day.  These consist mostly of non-functional requirements that do not often enjoy QA scrutiny.  The set of antipatterns and patterns is one that I think all developers, QA, and project managers would do well to consider within the scope of their current and future projects.</p>

<p>Overall, it was a thoughtful book.  I particularly enjoyed the sections on capactiy and stability that seemed more immediately applicable to my everday life in code.  I found the operations section to be the driest, but giving me opportunity to consider the plight of sys admins.  I would agree with the NFJS panel and highly recommend the read.</p>

<p>I have included a short slide deck on the stability section with some highlights on what one might consider and what is explained in much clearer detail in the book.</p>

<!--more-->




<iframe src="http://docs.google.com/present/embed?id=dcsq834g_58hp4kx2gx&size=m" frameborder="0" width="555" height="451"></iframe>



]]></content>
  </entry>
  
</feed>
