<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: unit-testing | Jake Trent]]></title>
  <link href="http://jaketrent.com/blog/categories/unit-testing/atom.xml" rel="self"/>
  <link href="http://jaketrent.com/"/>
  <updated>2015-09-23T16:30:25-06:00</updated>
  <id>http://jaketrent.com/</id>
  <author>
    <name><![CDATA[Jake Trent]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Testing React on jsdom]]></title>
    <link href="http://jaketrent.com/post/testing-react-with-jsdom/"/>
    <updated>2015-06-18T07:13:00-06:00</updated>
    <id>http://jaketrent.com/post/testing-react-with-jsdom</id>
    <content type="html"><![CDATA[<p>React allows you to create components that will render UI for your application.  If your UI is of any complexity, you'll likely want to test that it functions correctly and allows for future refactors.  There are numerous ways to do this.  One way that you might appreciate is using <a href="https://github.com/tmpvar/jsdom">jsdom</a>, an in-JavaScript implementation of the DOM.</p>

<p><img src="http://i.imgur.com/DXuSNbw.png" alt="react on jsdom" /></p>

<!--more-->


<h2>What is jsdom?</h2>

<p>Jsdom is an in-JavaScript implementation of the DOM.  The DOM is the document object model, which is the tree of nodes that make up the UI for documents shown in web browsers.</p>

<p>Because jsdom is implemented in JavaScript, we can have a DOM-like API to work with without needing a browser.  That means that we don't have to capture a browser in order test, a la <a href="http://karma-runner.github.io/">Karma</a>.  That means that we can run our tests in environments without browsers, like in Node or in continuous integration environments.</p>

<p>By not using real browsers, we're also essentially saying that we believe the problems in our client JavaScript will not be browser-dependent (again, because we're not capturing <em>real</em> browsers).</p>

<h2>jsdom Requirements</h2>

<p>The <a href="https://github.com/tmpvar/jsdom">latest version of jsdom</a>, as of this writing, requires <a href="https://iojs.org/en/index.html">io.js</a> instead of Node to run.  If you'd like to run in Node.js instead, the <a href="https://github.com/tmpvar/jsdom/tree/3.x">3.x series of jsdom</a> is required.</p>

<p>As <a href="http://thenextweb.com/dd/2015/06/16/node-js-and-io-js-are-settling-their-differences-merging-back-together/">Node and io.js merge in the future</a>, I expect the new versions of jsdom to support the one, unified platform.</p>

<h2>The Test Runner</h2>

<p>In my JavaScript tests, I prefer to use the <a href="http://mochajs.org/">Mocha</a> test runner.  It's usable in Node or browser environments.  It has great async test support.  It has the familiar behavior-style syntax of <code>describe</code> and <code>it</code> and all the normal test setup hooks that you like.  Most importantly, it features the Nyan cat test reporter:</p>

<p><img src="http://mochajs.org/images/reporter-nyan.png" alt="nyan cat test reporter" /></p>

<p>Mocha lets you choose your own assertion library.  I like <a href="http://shouldjs.github.io/">should.js</a>.</p>

<p>To install both:</p>

<p><code>
npm install mocha should --save-dev
</code></p>

<h2>Configuring Mocha</h2>

<p>By default, Mocha wants a <code>/test</code> directory in your project root in which to put its configuration and potentially your spec files, depending on how you like to organize your projects:</p>

<p><code>
mkdir test
</code></p>

<p>Inside of the <code>/test</code> dir, let's <code>vim mocha.opts</code> and give it a place to configure jsdom and our other libs:</p>

<p><code>text mocha.opts
--require test/utils/dom.js
--require should
--reporter nyan
</code></p>

<p>We have yet to write the <code>test/utils/dom.js</code> file.  We will.  This file includes options for mocha.  Line by line, we're saying that before mocha runs tests, we want to load our <code>dom.js</code> config, the <code>should.js</code> library, and specify our reporter as <code>nyan</code>.</p>

<h2>Configuring jsdom</h2>

<p>I've tried to come up with several configurations that will make jsdom work well with my React projects.  This is the best/simplest that I've come up with.  I'd be interested to hear if you have any suggestions on the approach.</p>

<p>Here's the full configuration with explanatory comments inline, as it would appear in <code>test/utils/dom.js</code>:</p>

<p>```js dom.js
var jsdom = require('jsdom')</p>

<p>// setup the simplest document possible
var doc = jsdom.jsdom('&lt;!doctype html><html><body></body></html>')</p>

<p>// get the window object out of the document
var win = doc.defaultView</p>

<p>// set globals for mocha that make access to document and window feel
// natural in the test environment
global.document = doc
global.window = win</p>

<p>// take all properties of the window object and also attach it to the
// mocha global object
propagateToGlobal(win)</p>

<p>// from mocha-jsdom https://github.com/rstacruz/mocha-jsdom/blob/master/index.js#L80
function propagateToGlobal (window) {
  for (let key in window) {</p>

<pre><code>if (!window.hasOwnProperty(key)) continue
if (key in global) continue

global[key] = window[key]
</code></pre>

<p>  }
}
```</p>

<p>The reason that we want to attach all the <code>window</code> properties to the mocha <code>global</code> object is because developers often write code that is meant for the browser without explicitly using the global environment object.  For instance, in React the developers write:</p>

<p><code>js
navigator.userAgent.indexOf('Chrome') &gt; -1
</code></p>

<p>instead of:</p>

<p><code>js
window.navigator.userAgent.indexOf('Chrome') &gt; -1
</code></p>

<p>Withing taking <code>window.navigator</code> and putting it on <code>global.navigator</code>, you'd get an error like this when running your tests:</p>

<p><code>
ReferenceError: navigator is not defined
</code></p>

<h2>The React Test</h2>

<p>You are now ready to render React components into a document in your tests.  The document will be provided by jsdom.  You don't need a browser environment to run this.</p>

<p>```js mycomponent.spec.js
var React = require('react/addons')
var should = require('should')
var TestUtils = React.addons.TestUtils
var MyComponent = // a React.Component with a <button/> ...
describe('MyComponent', function () {
  it('has button that fires a dom event for click', function (done) {</p>

<pre><code>function handleClick() { done() }
var detachedComp = TestUtils.renderIntoDocument(&lt;MyComponent onClick={handleClick}/&gt;)
var button = TestUtils.findRenderedDOMComponentWithTag(detachedComp, 'button')
var buttonNode = React.findDOMNode(button)
should.exist(buttonNode)
TestUtils.Simulate.click(buttonNode)
</code></pre>

<p>  })
})
```</p>

<p>To run mocha with the configuration above, I like to add an npm script in <code>package.json</code>:</p>

<p>```json package.json
{
   "scripts": {</p>

<pre><code> "test": "mocha test/**/*.spec.js"
</code></pre>

<p>   }
}
```</p>

<p>And then type:</p>

<p><code>bash
npm test
</code></p>

<p>Write the source to satisfy your test, and the lights should go green.  Your DOM is happy.  Your test is happy.</p>

<p>Does it work well for you?  How might you improve on it?  Enjoy testing!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Test React componentWillReceiveProps]]></title>
    <link href="http://jaketrent.com/post/test-react-componentwillreceiveprops/"/>
    <updated>2015-02-26T08:34:00-07:00</updated>
    <id>http://jaketrent.com/post/test-react-componentwillreceiveprops</id>
    <content type="html"><![CDATA[<p>Testing React Components has been easier and more enjoyable than any previous UI unit testing I've done in the past.  Components that have interesting things happen in lifecycle methods have a little more setup to get tested.  Components that use the <code>componentWillReceiveProps</code> method are in this category.</p>

<p><img src="http://i.imgur.com/DXuSNbw.png" alt="react" /></p>

<!--more-->


<h2>React Test Setup</h2>

<p>Not all lifecycle methods require as much setup in a test as <code>componentWillReceiveProps</code>.  This is because:</p>

<ul>
<li>This method is concerned with <strong><em>changing</em></strong> props.</li>
<li>Changing props directly on a React Component (even under test) is against the React code of conduct</li>
</ul>


<p>So, we need something that is legal to change... <code>state</code>!  We need to not modify our subject under test and simply pass it new <code>props</code>.</p>

<p>My solution is to create a React Component specifically for the test.  This Component will be a parent to the subject under test, on which we can set <code>state</code>.  We'll design it so that this state is transferred to the child Component under test.</p>

<h2>React Component using <code>componentWillReceiveProps</code></h2>

<p>You might have a React Component to test that looks like this:</p>

<p>```js
var ComponentToTest = React.createClass({
  getInitialProps() {</p>

<pre><code>return {
  myProp: "blank"
};
</code></pre>

<p>  },
  getInitialState() {</p>

<pre><code>return {
  modified: "still blank"
};
</code></pre>

<p>  },
  componentWillReceiveProps(nextProps) {</p>

<pre><code>this.setState({
  modified: nextProps.myProp + "IsSoModified"
});
</code></pre>

<p>  },
  render() {</p>

<pre><code>return &lt;div class="displayed"&gt;{this.state.modified}&lt;/div&gt;
</code></pre>

<p>  }
});
```</p>

<p>There is nothing particularly interesting about this subject beyond the fact that it uses <code>componentWillReceiveProps</code>.  When new props are received, internal state is modified.  In real life, more interesting things like data fetching or complex calculations might be done here and then stored in state.  We simply are matching the scenario of needing to verify something when <code>componentWillReceiveProps</code> is called.</p>

<h2>A Parent Test Component</h2>

<p>The test to exercise <code>componentWillReceiveProps</code> on the above Component might look like this:</p>

<p>```js
var React = require("react/addons");
var TestUtils = React.addons.TestUtils;</p>

<p>it("displays a modified state upon changing props", function () {
  var TestParent = React.createFactory(React.createClass({</p>

<pre><code>getInitialState() {
  return { testState: "init" };
},
render() {
  return &lt;ComponentToTest ref="sot" myProp={this.state.testState} /&gt;
}
</code></pre>

<p>  }));</p>

<p>  var parent = TestUtils.renderIntoDocument(TestParent());
  parent.refs.sot.props.myProp.should.eql("init");</p>

<p>  parent.setState({</p>

<pre><code>testState: "somethingElse"
</code></pre>

<p>  });</p>

<p>  parent.refs.sot.props.myProp.should.eql("somethingElse");
  parent.refs.sot.state.modified.should.eql("somethingElseIsSoModified"); // assert #1
  var child = TestUtils.scryRenderedDOMComponentsWithClass(parent, "displayed")[0];
  child.getDOMNode().innerText.should.eql("somethingElseIsSoModified");  // assert #2
});
```</p>

<p>The <code>TestParent</code> component is created specifically for this test.  It renders the Component under test.  It sets a <code>ref</code> attribute to it for easy access.  Once we render the parent, the initial state was sent as the prop to the child Component.  As soon as we <code>setState</code> on the parent, a new prop is sent to the child, triggering <code>componentWillReceiveProps</code>.</p>

<p>Finally, I've included two assertion styles.  Again, there are going to be more interesting things that you're asserting here in real life.  In this case I'm verifying that state that renders directly to the UI is set in our lifecycle method.  I can interrogate the state directly.  Here, we reach into the child Component state for assertion method #1.  That may sound bad, but remember that <code>TestParent</code> was created only in the context of this test anyway, so the level we're reaching through to grab child state is just test code.  Assertion method #2 is to go to the DOM to verify final output from the state change.</p>

<p>There is definitely more setup here to make this happen.  I feel like usually testing a React Component doesn't require this much test code.</p>

<h2><code>componentWillReceiveProps</code> in Action</h2>

<p>Here is a Component that uses <code>componentWillReceiveProps</code> to make a simple display change:</p>

<p><a class="jsbin-embed" href="http://jsbin.com/munaxuguta/12/embed?js,output">JS Bin</a><script src="http://static.jsbin.com/js/embed.js"></script></p>

<p>You can also check out this <a href="http://jsbin.com/buwoqod/22/edit?js,output">slightly modified jsbin</a> which does the test assertion.</p>

<p>What methods have you used to test Components that use either <code>componentWillReceiveProps</code> or other interesting lifecycle methods?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sinon Spies vs. Stubs]]></title>
    <link href="http://jaketrent.com/post/sinon-spies-vs-stubs/"/>
    <updated>2015-02-25T07:35:00-07:00</updated>
    <id>http://jaketrent.com/post/sinon-spies-vs-stubs</id>
    <content type="html"><![CDATA[<p><a href="http://sinonjs.org/">Sinon</a> provides spies, stubs, and mocks.  They're all useful as fakes in tests.  They come with essential differences for what they're helpful in doing and how they work.</p>

<p><img src="http://i.imgur.com/yuKcrP9.jpg" alt="spies vs stubs" /></p>

<!--more-->


<h2>Why Use Fakes?</h2>

<p>In a unit test, you might want to avoid having to test the unit's dependencies.  This is especially true in <a href="http://en.wikipedia.org/wiki/White-box_testing">white-box testing</a>.  In this case, test fakes are going to be very helpful.  Sinon provides several fakes, notably spies, stubs, and mocks.  Let's compare and contrast the three:</p>

<h2>Sinon Spies</h2>

<p>Spies sound like what they do -- they watch your functions and report back on how they are called.  They generally avoid the violence and mayhem of a Hollywood spy, but depending on your application, this could vary.</p>

<p>They don't change the functionality of your application.  They simply report what they see.  The <a href="http://sinonjs.org/docs/#spies-api">sinon API for spies</a> is fairly large, but it essentially centers around the <code>called</code> attribute (of which there are many variations).</p>

<p>I first setup that I want to spy on something.  Then I call my subject under test (src code).  Then I verify with the spy what was actually called and stop spying.  That might look like this in a test:</p>

<p>```js
describe("#fight", function () {
  it("calls prayForStrength for fight success", function () {</p>

<pre><code>sinon.spy(subject.strengthDep, "prayForStrength");
subject.fight();
subject.strengthDep.called.should.be.true;
subject.strengthDep.restore();
</code></pre>

<p>  });
});
```</p>

<p><em>Note: this example is in <a href="http://mochajs.org/">mocha</a> using a <a href="https://www.npmjs.com/package/should">should.js</a> assertion style</em></p>

<p>The dependency's <code>prayForStrength</code> method is referred to by name in a string to setup the spy.  When <code>fight</code> is called here, <code>strengthDep.prayForStrength</code> will be called as normal -- but there will be someone watching.  Finally, we call <code>restore</code> on the function we spied on so that all spies are called off.  If you want to do more than watch as dependencies work as described, you might want to use a stub.</p>

<h2>Sinon Stubs</h2>

<p>Stubs are more hands-on than spies (though they sound more useless, don't they).  With a stub, you will actually change how functions are called in your test.  You don't want to change the subject under test, thus changing the accuracy of your test.  But you may want to test several ways that dependencies of your unit could be expected to act.</p>

<p>For instance, if you had a function that returned a boolean that your code used to do different things, you might want to use a stub in two different tests to verify conditions when returning different values (ie, guarantee one run of <code>true</code> and one of <code>false</code> return).</p>

<p>To continue the <code>fight</code> example from above, let's assume that if <code>prayForStrength</code> returns true, we are guaranteed to win the fight for the orphans (ie, <code>fight()</code> should return <code>true</code>).  That might look like this:</p>

<p>```js
describe("#fight", function () {
  it("always wins when prayForStrength is true", function () {</p>

<pre><code>sinon.stub(subject.strengthDep, "prayForStrength", function () { return true; });
subject.fight().should.be.true;
subject.strengthDep.restore();
</code></pre>

<p>  });
});
```</p>

<p>Notice that we use a different <code>sinon.stub</code> API.  For the 3rd parameter, we're supplying our own version of <code>prayForStrength</code>.  For our test, all we care about is the return value, so that's all we supply.  We're not testing this dependency.  We're instead testing how our subject <code>fight</code>s in a certain circumstance.  There are many ways you can use <a href="http://sinonjs.org/docs/#stubs">sinon stubs</a> to control how functions are called.  Also note that you can still use the <code>called</code> verifications with stubs.  But if you do verify a stub was called, you may want to use a mock.</p>

<h2>Sinon Mocks</h2>

<p><a href="http://sinonjs.org/docs/#mocks">Mocks</a> take the attributes of spies and stubs, smashes them together and changes the style a bit.  A mock will both observe the calling of functions and verify that they were called in some specific way.  And all this setup happens <em>previous</em> to calling your subject under test.  After the call, mocks are simply asked if all went to plan.</p>

<p>So the previous test could be rewritten to use a mock:</p>

<p>```js
describe("#fight", function () {
  it("always wins when prayForStrength is true", function () {</p>

<pre><code>var mock = sinon.mock(subject.strengthDep)
mock.expects("prayForStrength").returns(true);
subject.fight().should.be.true;
mock.verify();
mock.restore();
</code></pre>

<p>  });
});
```</p>

<p>The <code>expects</code> and <code>returns</code> line is where the combo magic happens.  <code>expects</code> is verifying a call (like <code>spies</code> can), and <code>returns</code> is specifying functionality (like <code>stubs</code> can).  The <code>verify</code> call is the line that will fail (essentially the mock assertion) if things in the subject didn't go exactly according to plan.</p>

<h2>Spies vs. Stubs vs. Mocks</h2>

<p>So when should I use spies or stubs or mocks?  As with most art, there are many ways to accomplish what you want.  Much of your choice will depend on your own style and what you become proficient in.</p>

<p>Some basic rules might be:</p>

<ul>
<li><strong><em>Use Spies</em></strong> - if you simply want to watch and verify somethings happens in your test case.</li>
<li><strong><em>Use Stubs</em></strong> - if you simply want to specify how something will work to help your test case.</li>
<li><strong><em>Use Mocks</em></strong> - if you want to both of the above on a single dependency in your test case.</li>
</ul>


<p>When do you find yourself most often using spies vs. stubs vs. mocks?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set Cookie on Rack Mock Request]]></title>
    <link href="http://jaketrent.com/post/set-cookie-on-rack-mock-request/"/>
    <updated>2015-01-21T10:58:00-07:00</updated>
    <id>http://jaketrent.com/post/set-cookie-on-rack-mock-request</id>
    <content type="html"><![CDATA[<p>Rack apps are generally straightforward to test because of their very basic public interface.  But where do we put specific things, in this case, a cookie for the request, on that <code>env</code> argument it takes?  Here's one way.</p>

<p><img src="http://i.imgur.com/044x6s6.jpg" alt="rack mock request" /></p>

<!--more-->


<h1>env</h1>

<p>The <code>env</code> argument that is sent to <code>#call</code> in a Rack app is an variable that represents the <a href="http://www.rubydoc.info/github/rack/rack/master/file/SPEC#The_Environment">environment of the request</a>.  It is a hash of CGI-like headers: request method, query params, http headers -- that sort of thing.</p>

<p>The <code>HTTP_*</code> keys on the hash will be <a href="https://tools.ietf.org/html/rfc3875#section-4.1.18">read as request headers from the environment</a>.</p>

<h1>Rack MockRequest Cookies</h1>

<p>Rack provides a great little <code>Rack::MockRequest</code> helper object in its library that will help us test our app.  This object has a class method called <code>env_for</code> which allows for quick construction of an <code>env</code> var that is a request to a specified url.</p>

<p>On the returned <code>env</code> we will continue to make modifications before passing it to our Rack app.  We'll add our cookie header with a key of <code>HTTP_COOKIE</code>.  The value will be of the format <code>cookieName=cookieValue</code>.  We can handle multiple cookies by separating the cookies with <code>;</code>.</p>

<p>Let's say we're testing a token authentication middleware with rspec.  Our spec may look something like this:</p>

<p><code>ruby
it "accepts an cookie token in the request" do
  middleware = # ... instantiate rack middleware
  env = Rack::MockRequest.env_for("/protected")
  env["HTTP_COOKIE"] = "AUTH_COOKIE=123"
  status, _, _ = middleware.call(env)
  expect(status).to eq(200)
end
</code></p>

<p>Then in our Rack app source, we can code for the availability of a cookie on the request.  It might look like:</p>

<p>```ruby</p>

<h1>...</h1>

<p>def call(env)
  request = Rack::Request.new(env)
  token = request.cookies["AUTH_TOKEN"]
  # ...
end
```</p>

<p>Is this the easiest or best way to set cookies on requests when testing Rack apps?  What do you do?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Destroy Duplicate Tests]]></title>
    <link href="http://jaketrent.com/post/destroy-duplicate-tests/"/>
    <updated>2014-11-05T16:59:00-07:00</updated>
    <id>http://jaketrent.com/post/destroy-duplicate-tests</id>
    <content type="html"><![CDATA[<p>As soon as we begin to write a test for our code, it is natural for us to think that we are doing a good thing, and often, we are.  Yet, I believe there are times that we’re writing tests when we’re hurting more than helping — and, of course, this is not on purpose.  To clarify, I’m an advocate for testing in general, and this is a short thought on how to make it better.</p>

<p><img src="http://i.imgur.com/ozzuTNQ.png" alt="Double tests are not fine" /></p>

<!--more-->


<p>As soon as we begin to write a test for our code, it is natural for us to think that we are doing a good thing, and often, we are.  Yet, I believe there are times that we’re writing tests when we’re hurting more than helping — and, of course, this is not on purpose.  To clarify, I’m an advocate for testing in general, and this is a short thought on how to make it better.</p>

<h2>Verify It, and Be Done</h2>

<p>One of the main goals of testing is to verify that what you have written is correct.  So, if we’ve met that goal, there’s no need to go around the track one more time and see the checkered flag again.  The second time around produces no extra glory and no extra benefit.</p>

<p>If we cover a section of code many times, it isn’t more helpful than the first time we covered it.  To verify twice isn’t to verify any better.  If the second attempt does happen to verify the same thing in an obviously better way, remove the first attempt and keep the second.</p>

<p>If it’s a variation of a certain case that you’re verifying, that’s different.  Adding new cases based on slight permutations of previous cases can be a good thing.  But covering the exact same thing provides no value.  In fact, multiple verification of code is just a type of debt.  It should be a smell in your test code that alerts you to clean things up.</p>

<h2>The Debt of Duplicate Tests</h2>

<p>If you have multiples of something, it just increases the maintenance over time.  Why would you want to update two tests instead of one?  Now that you have duplicate tests, you also have to keep them in sync.  Of course, if they cover the exact same case, if you change source code to fix the one test, the other will still be broken and be apparent and easy to fix.</p>

<p>The more tests you have, the longer your feedback loop in development or in a continuous build environment will be.  Multiply that extra wait time across your life on the project, and it has the possibility of being a non-trivial product.  Of course we need to wait for the tests that are needful and provide added value, but we shouldn’t wait needlessly.</p>

<p>Sometimes you do see duplicate tests within the same file — for instance, within the same unit.  This might happen when different developers approach the unit at different times to add tests.</p>

<p>I think it’s probably more often the case that duplicate tests are found across test classes -- meaning across the different types of tests.  For instance, a developer might write a unit test that covers a case.  Later, someone else might add an integration test that adds the same case.  Later still, someone else might add a functional test that adds the same case yet again.  All these developers are well-intentioned in adding tests.  They all need to think, communicate, investigate, and coordinate a little more to avoid the duplicate test problem.</p>

<h2>Deleting Duplicate Tests</h2>

<p>When duplicate tests are found, we should delete them.  Again, this might require some thinking.  We might want to consider which of the duplicate cases is the best test and therefore the one to keep.  This consideration might include which test is most stable, runs the fastest, is most readable, best designed, latest, earliest, etc.</p>

<h2>Avoiding Duplicate Tests</h2>

<p>The best scenario would be the one where we avoid duplicate tests.  Teams with clear guidelines will be able to coordinate better.  Useful information might include which classes of tests exist in the project and what each is intended for.  We might describe which kinds of tests we prefer, in which order, for certain kinds of verifications.  Having clean, well-organized tests will also encourage the team to read each others’ tests and familiarize themselves with what’s already written and know where to find existing cases and where to properly categorize new cases.</p>

<p>So have fun testing, and destroy the duplicate tests!  Yay for test doubles, but boo for double tests.</p>
]]></content>
  </entry>
  
</feed>
